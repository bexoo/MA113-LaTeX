\documentclass[12pt]{article}
% EXAM TUESDAY 6PM

% Include math
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{physics}
% Include links
\usepackage{hyperref}
% Cool epigraph style stuff
\usepackage{epigraph}

%%%%%%%%%%%%%  THEOREMS  %%%%%%%%%%%%%%%%%
\theoremstyle{plain} % other options: definition, remark
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem*{claim}{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Remark}
% By including [theorem], the lemma follows the numbering of theorem
% e.g. Thm 1, Lemma 2, Thm 3, Thm 4, \dots
\theoremstyle{definition}
\newtheorem*{definition}{Definition} % the star prevents numbering

\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]

% Remarks
\theoremstyle{remark}
%\newtheorem{remark}{Remark}




%%%%%%%%%%%%%%  PAGE SETUP %%%%%%%%%%%%%%%%%
% LaTeX has big default margins
% The following sets them to 1in
\usepackage[margin=1.5in]{geometry}

\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}
\setlength{\headheight}{15.0pt}
\renewcommand{\epigraphflush}{center}

% The following sets up some headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{MA113 Fall 2022} % Left Header
\rhead{Brady Exoo} % Right Header
\cfoot{\thepage} % Center Foot (empty)
\setlist[itemize,1]{label=\textbullet}\setlist[itemize,2]{label=$\circ$}




%%%%%%%%%%%%% SHORTCUTS %%%%%%%%%%%%%%%%%%%%
\newcommand{\half}{\frac{1}{2}}
\newcommand{\vecv}{\vv{v}}
\newcommand{\vecu}{\vv{u}}
\newcommand{\vecw}{\vv{w}}
\newcommand{\vecn}{\vv{n}}
\newcommand{\vecr}{\vv{r}}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\angled}[1]{\left\langle {#1} \right\rangle}
\newcommand{\cbrt}[1]{\sqrt[3]{#1}}
\newcommand\normx[1]{\Vert#1\Vert}
\renewcommand\abs[1]{\left| #1 \right|}
\newcommand*{\vv}[1]{\overrightarrow{\mkern0mu#1}}
\renewcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\im}{\hat{i}}
\newcommand{\jm}{\hat{j}}
\newcommand{\km}{\hat{k}}
\newcommand{\ddt}{\frac{d}{dt}}
\newcommand{\ddx}{\frac{d}{dx}}
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\pypx}{\frac{\partial y}{\partial x}}
\newcommand{\ppx}{\frac{\partial}{\partial x}}






% Document content begins here
\begin{document}

% Set up a title
\title{MA113 Notes}
\author{Brady Exoo}
\date{September 1, 2022 - November 17, 2022}
\maketitle

\begin{abstract}
Notes for MA113 (Mutlivariable Calculus) taught by Dr. Holder.
\end{abstract}

% This line makes a ToC
\tableofcontents

% This line starts a new page
\eject

\section{Coordinate Systems and Polar Functions}
\epigraph{\itshape ``This is your last chance. After this there is no turning back. You take the blue pill, the story ends. You wake up in your bed and believe whatever you want to. You take the red pill, you stay in Wonderland, and I show you how deep the rabbit hole goes.''}{Morpheus, \textit{The Matrix}}
We begin with a discussion of some two-dimensional coordinate systems.

\subsection{Rectangular Coordinates}
\begin{itemize}
    \item $(x,y)$
    \item Every point has only one set of coordinates
    \item Unique representation!
\end{itemize}

\subsection{Polar Coordinates}
\begin{itemize}
    \item $(r,\theta)$
    \begin{itemize}
        \item $r$ is the distance from the origin
        \item $\theta$ is the angle the radius faces
    \end{itemize}
    \item Lacks unique representation
    \begin{itemize}
        \item Can add $2\pi$ to any angle and get the same point
        \item Can make radius negative and add $\pi$ to the angle and get the same point
    \end{itemize}
\end{itemize}
These coordinates will be helpful for setting up integrals later in the course. Often by a change of coordinates we turn a complicated integral into an elegant one!

\subsection{Transformations}
\definition{$\tan{\theta}$ is the distance from the intersection of the extended radius and the vertical tangent to the tangent point.}
\begin{itemize}
    \item $x^2+y^2=r^2$
    \item $\tan{\theta} = \frac{y}{x}$
    \begin{itemize}
        \item $x=0 \implies \theta \in \{\frac{\pi}{2}, \frac{3\pi}{2}\}$
    \end{itemize}
    \item $\theta = \tan^{-1}{\frac{y}{x}}$
    \begin{itemize}
        \item $\tan{\theta}$ is the distance from the intersection of the extended radius and the vertical tangent to the tangent point
        \item This is problematic! Range of arctan is $(\frac{-\pi}{2},\frac{\pi}{2})$, so we do not get all $360$ degrees.
        \item $ \theta = \begin{cases}
            \tan^{-1}{\frac{y}{x}} & x\leq 0 \\
            \frac{\pi}{2} & x=0,y>0 \\
            \frac{3\pi}{2} & x=0,y<0 \\
            \tan^{-1}{\frac{y}{x}} + \pi & x < 0
            \end{cases}
        $
    \end{itemize}
    \item $r\cos{\theta} = x$
    \item $r\sin{\theta} = y$
\end{itemize}

\subsection{Polar Graphs}
\begin{itemize}
    \item Graphs are of the form $r(\theta)$, $r$ is a function of $\theta$.
    \item $r(\theta) = \cos{\theta}-1$ is a cardioid.
    \item $r(\theta)=\cos{(2\theta)}$ is a $4-$flower.
    \begin{itemize}
        \item $\cos{(\text{n}\theta)}$ has $2n$ petals if $n$ is even, $n$ petals if $n$ is odd.
    \item $r = \theta$ is the Archimedes spiral.
    \begin{itemize}
        \item This can be generalized by $r = a\theta$.
    \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Differentiating Polar Functions}
\begin{itemize}
    \item We come to a problem, we wish to find $\frac{dy}{dx}$ but our functions are in terms of $r$ and $\theta$!
    \item $\frac{dy}{dx}$ = $\frac{dy/d\theta}{dx/d\theta}$ by the Chain Rule.
    \begin{itemize}
        \item Not because we can cancel out the $d\theta$ terms!
    \end{itemize}
\end{itemize}

\subsubsection{Chain Rule}
The default way the Chain Rule is portrayed is
\[ \frac{d}{dx} f(g(x)) = f^\prime(g(x)) \cdot g^\prime(x) \text{.} \]
Other notation is
\[ \frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx} \text{.} \]
The Chain Rule allows us to change variables which we do not wish to differentiate by.
\subsubsection{Polar Derivatives}
Now we can substitute $y$ and $x$ with our earlier transformations to get
\[ \frac{dy}{dx} = \frac{\frac{d}{d\theta}r\sin{\theta}}{\frac{d}{d\theta}r\cos{\theta}} \text{.} \]

\subsection{Arc Lengths}
\subsubsection{Review}
In Calculus II, we learned the arclength of a function from $a$ to $b$ as
\[ \int_a^b{\sqrt{1+(f^\prime(x))^2}dx} \]
This is somewhat related to
\[ \frac{d}{dx} \langle x,f(x) \rangle = \langle 1,f^\prime(x) \rangle \]
\[ || \langle1, f^\prime(x) \rangle || = \sqrt{1+(f^\prime(x))^2} \text{.} \]
Here we are essentially integrating the magnitude of the derivative of the vector, which gives us the arclength (think about this visually with the Pythagorean Theorem).
We can also think of this as ``vectorizing'' the derivative.

\subsubsection{Polar Arc Length}
We want to find the arc length of a function in terms of $r(\theta)$.
\begin{align*}
    s & = \int_{\theta_1}^{\theta_2}{\lVert \frac{d}{d\theta} \langle r\cos{(\theta)}, r\sin{(\theta)} \rangle \rVert d\theta} \\
    & = \int_{\theta_1}^{\theta_2}{|| \langle r^\prime\cos{(\theta)} - r\sin{(\theta)}, r^\prime\sin{(\theta)} + r\cos{(\theta)} \rangle || d\theta} \\
    & = \int_{\theta_1}^{\theta_2}{\sqrt{ (r^\prime\cos{(\theta)} - r\sin{(\theta)})^2+ (r^\prime\sin{(\theta)} + r\cos{(\theta)})^2  } d\theta} \\
    & = \int_{\theta_1}^{\theta_2}{\sqrt{r^2+ (r^\prime)^2}d\theta}
\end{align*}
This is the same as the previous arc length formula, just with substitutions.

\subsection{Polar Area}
We integrate all the sectors of the function, each of which approximates the sector of a circle,
each of which has angle $\Delta \theta$ and radius $r(\theta)$. The area of each sector is
\begin{equation*}
    (\pi r^2)(\frac{\Delta\theta}{2\pi}) = \frac{1}{2}r^2 \Delta\theta.
\end{equation*}
Thus, the total area is
\[
    \frac{1}{2} \int_{\theta_1}^{\theta_2}{r(\theta)^2 d\theta}.
\]
We can subtract these integrals as needed to find areas between two curves:
\[
    \frac{1}{2} \int_{\theta_1}^{\theta_2}{r_2(\theta)^2 - r_1(\theta)^2 d\theta}.
\]

\section{Vectors}
\subsection{Fundamentals}
\begin{itemize}
    \item The point $A(1,2)$ to the point $B(6,5)$ is denotated as the vector $\overrightarrow{AB} = \langle 6-1,5-2 \rangle = \langle 5,3 \rangle$.
    \item Vectors have magnitude and direction.
    \item Vectors in angle bracket notation are typically assumed to start at the origin.
    \item Arrow hats represent vectors, some books boldface instead.
    \begin{itemize}
        \item Column vectors are also used: $\begin{bmatrix} 5 \\ 3 \end{bmatrix}$
    \end{itemize}
    \item Adding vectors is the same as adding their components.
    \begin{itemize}
        \item Geometrically, we add tail of one vector to the head of another.
        \item Can only add vectors in the same number of dimensions.
        \item $\langle 1,2,0 \rangle + \langle -1,4,8 \rangle = \langle 0,6,3 \rangle$
    \end{itemize}
    \item In subtraction we add the negative of the vector.
    \item Scalar multiplication of a vector is the same as multiplying each component by that scalar.
    \begin{itemize}
        \item $3\angled{1,2,0} = \angled{3,6,0}$
    \end{itemize}
\end{itemize}

\subsection{Norms}
\begin{itemize}
    \item Norms are the length or magnitude of a vector
    \begin{itemize}
        \item Notated as $||\overrightarrow{v}||$.
    \end{itemize}
    \item $|| \overrightarrow{v} || = \sqrt{v_1^2 + v_2^2 + \dots + r_n^2}$.
    \item $\angled{1,3,2} = \sqrt{14} \angled{\frac{1}{\sqrt{14}},\frac{3}{\sqrt{14}},\frac{2}{\sqrt{14}}}$.
    \begin{itemize}
        \item Generally, this is useful because we separate the norm from the direction (unit vector).
        \item General form is $|| \vec{v} || (\frac{\vec{v}}{||\vec{v}||})$.
        \item This is called normalizing a vector.
    \end{itemize}
    \item Special Vectors
    \begin{itemize}
        \item $\hat{i} = \angled{1,0,0}$
        \item $\hat{j} = \angled{0,1,0}$
        \item $\hat{k} = \angled{0,0,1}$
        \item Always assume we are in three dimensions (every vector has $0\hat{k}$).
    \end{itemize}
\end{itemize}

\subsection{Dot Product}
The dot product is defined as
\[ \vv{v} \cdot \vv{w} = v_1w_1 + v_2w_2 + \dots + v_nw_n. \]
This has a lot of cool properties, for one: \[\vv{v} \cdot \vv{w} = ||\vv{v}||\,||\vv{w}||.\]
Also, the dot product is commutative and can be distributed.
\begin{exmp}
    $\angled{1,3,2} \cdot \angled{3,0,2} = 3+0+2 = 7$.
\end{exmp}

\subsubsection{Angles in the Dot Product}
Additionally, by the law of cosines on our vector subtraction example, we get:
\begin{align*}
||\vv{v}||^2 + ||\vv{u}||^2 - 2||\vv{u}|| ||\vv{v}|| \cos{\theta} & = ||\vv{v}-\vv{u}||^2 \\
& = (\vv{v}-\vv{u})\cdot(\vv{v}-\vv{u}) \\
& = \vv{v} \cdot \vv{v} - 2 \vv{u} \cdot \vv{v} + \vv{u} \cdot \vv{u}
\end{align*}
Simplifying, we get
    \[\cos{\theta} = \frac{\vv{u} \cdot \vv{v}}{||\vv{u}|| \, ||\vv{v}||}\]
This may also be represented as
\[ ||\vv{u}|| \, ||\vv{v}|| \cos{\theta} = \vv{u} \cdot \vv{v} \]
So, the dot product gives us:
\begin{itemize}
    \item Norms
    \item Angles
    \item Distance (metric)
\end{itemize}

\subsubsection{Projections}
We wish to find the component of one vector $\vv{u}$ onto another vector $\vv{v}$.
This is written as $\text{proj}_{\vv{v}}\vv{u}$, which is the projection of $\vecu$ onto $\vecv$.
This projection will be a scalar multiple of $\vecv$ (the vector we project onto),
which can be represented as $\text{proj}_{\vv{v}}\vv{u} = \alpha \vecv$.
Recall that
\[\cos{\theta} = \frac{\vv{u} \cdot \vv{v}}{||\vv{u}|| \, ||\vv{v}||}.\]
In our case $\theta=90^\circ$, so $\vecu \cdot \vecv = 0$.
Note that $\vecu \perp \vecv \: \iff \: \vecu \cdot \vecv = 0.$
We know that
\begin{align*}
    \alpha \vecv \cdot (\vecu - \alpha \vecv) & = 0 \\
    \vecu & = \alpha \vecv \cdot \vecv \\
    \alpha &= \frac{\vecv \cdot \vecu}{\vecv \cdot \vecv}.
\end{align*}
Therefore,
\[ \text{proj}_{\vecv} \vecu = (\frac{\vecv \cdot \vecu}{\vecv \cdot \vecv})\vecv .\]
A way to remember this is that $\vecv$ is the vector we are projecting onto.

\subsection{Three Dimensions}
We orient our axes by the right hand rule, where the $x$ axis is coming towards us.

\subsubsection{Set Notation}
\begin{itemize}
    \item A sphere would be $\{\vecu : \norm{\vecu} = k\}$.
    \begin{itemize}
        \item $x^2+y^2+z^2=k^2$
    \end{itemize}
    \item A sphere centered at another point would be $\{\vecu: \norm{\vecu-\vv{w}} = k\}$,
where $\vv{w}$ is the vector from the origin to the center of the sphere.
    \begin{itemize}
        \item $(x-a)^2+(y-b)^2+(z-c)^2=k^2$
    \end{itemize}
\end{itemize}

\subsection{Cross Product}
\begin{definition}
    $\vecv \times \vv{w} = \norm{\vecv} \, \norm{\vv{w}} \sin{\theta} \, \vv{n}$,
    where $\vv{n}$ is a unit vector which satisfies the right-hand rule.
\end{definition}
$n$ must be perpendicular to both vectors $\vecv$ and $\vv{w}$.
Alternatively,
\[  \norm{\vecv \times \vv{w}} = \norm{\vecv} \, \norm{\vv{w}} \, |\sin{\theta}| \]

\subsubsection{Properties}
\begin{itemize}
    \item Not commmuative, can be clearly seen by the right-hand rule, which gives opposite values if we reverse the order.
    \begin{itemize}
        \item $\vecv \times \vecw = -(\vecw \times \vecv)$
    \end{itemize}
    \item The magnitude of the cross product is the area of the parallelogram formed by the two vectors.
    \item $\vecv \times (\alpha \vecv) = \angled{0,0,0}$
    \item $\norm{\vecv} \, \norm{\vecw} = \vecv \times \vecw \iff \vecv \perp \vecw.$
    \begin{itemize}
        \item This means $\vecv \perp \vecw \iff \vecv \cdot \vecw = 0$.
    \end{itemize}
\end{itemize}

\subsection{Matrices}
\subsubsection{2 by 2}
Given a matrix
\[
\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix}
\]
The determinant of the matrix is $AD - BC$. This is the area of the parallelogram formed by the vectors $\angled{A,B}$ and $\angled{C,D}$.

\subsubsection{3 by 3} \label{3x3d}
Given a matrix
\[
\begin{bmatrix}
    A & B & C \\
    D & E & F \\
    G & H & I
\end{bmatrix}
\]
We first take $A$ and multiply it by the determinant of the matrix formed by crossing out its row and column (any place a rook could be placed and not hit $A$).
\[ A(EI-FH) \]
Every other term is negative, so we subtract the $B$ term and add the $C$ term, giving us
\[ A(EI-FH) - B(DI-FG) + C(DH - GE) \footnote{We can calulate this easier by expanding by a row (or column) with 0s, we don't always need to do the top row! The process will be the same, and the $+$s and $-$s form a checkerboard pattern.}\] as the determinant of this matrix. This is the volume of the parallelepiped formed by the vectors $\angled{A,B,C}$, $\angled{D,E,F}$ and $\angled{G,H,I}$. This is also known as the triple product, but it is really just
\[\angled{A,B,C} \times \angled{D,E,F} \cdot \angled{G,H,I}.\]

\subsubsection{Calculating the Cross Product With Matrices}
To calculate the cross product of $\vecv = \angled{0,1,2}$ and $\vecw = \angled{1,1,3}$ we use the determinant.
\[ \vecv \times \vecw = \det\footnote{This is slightly a misuse of notation, usually we should not have a vector in this matrix.}{\left(
\begin{bmatrix}
    \hat{i} & \hat{j} & \hat{k} \\
    0 & 1 & 2 \\
    1 & 1 & 3
\end{bmatrix}\right)} \]
We can dot the resulting vector with any of our original vectors and we should get $0$, this verifies that our calculation is correct.

\subsection{Parametric Equations}
Generally, a parametric function is one that takes a variable $t$ and maps it to a point $(x,y,z)$. They are usually used to model the position of moving objects. It can also be thought of as mapping a straight line onto a flexible line in space.

\subsubsection{Some Notation}
Usually we write functions as $y(x) = x^2$, however a more specific notation would be
\[ y:\mathbb{R} \rightarrow \mathbb{R}_+ : x \mapsto x^2 \]
This means we have a function $y$ such that it maps real numbers onto positive real numbers, and it maps a value $x$ onto $x^2$.
In this way, we can write a parametric function as
\[ \vv{r} : \mathbb{R} \rightarrow \mathbb{R}^3 : t \mapsto  \angled{x(t),y(t),z(t)} \]

\subsubsection{Crossing Sets}
We take the Cartesian product (similar to the cross product) of sets like this.
\[ \{1,2,3\}\times\{a,b\} = \{(1,a),(1,b),(2,a),(2,b),(3,a),(3,c)\} \]
In this same way,
\[ \mathbb{R}^2 = \mathbb{R} \times \mathbb{R} .\]
Also,
\[ \mathbb{R}^3 = \mathbb{R} \times \mathbb{R} \times \mathbb{R} .\]

\subsection{Lines in 3 Dimensions}
Defining lines as $y=mx+b$ does not work in three dimensions, since a line in three dimensions does not have a slope.. Instead, we define lines with a point and a direction.
\[ \vv{r}(t) = \vv{A} + t\vecv \]
where $\vv{A}$ is the origin point of the line and $\vecv$ is the direction of the line, which we can think of as taking the place of the slope. Usually we combine these two terms into a single vector, like
\[ \angled{2-t,1-2t,4-4t} .\]
Alternatively, we may write this as
\begin{align*}
    x(t) & = 2-t \\
    y(t) & = 1-2t \\
    z(t) & = 4-4t.
\end{align*}
The former is the vector form, and the latter equations are parametric equations.

\subsection{Distance Between Line and Point}
We wish the find the distance $d$ between a line and a point. The line is defined by a point $A$ and a vector $\vecv$, and the point is $B$. Let $\theta$ be the angle between the vectors $\vecv$ and $\vv{AB}$. We quickly see that
\[ \sin{\theta} = \frac{d}{\norm{\vv{AB}}} .\]
Now we may see hints of the cross product in here.
\begin{align*}
    d & = \frac{\norm{\vecv}\norm{\vv{AB}}\sin{\theta}}{\norm{\vecv}} \\
     & = \boxed{\frac{\norm{\vv{AB} \times \vecv}}{\norm{\vecv}}}.
\end{align*}

\subsection{Planes}
A plane is defined as, given a point $A$ and a vector $\vecv$ rooted at $A$, the set of all points $P$ such that $\vv{AP} \perp \vecv$, or $\vv{AP} \cdot \vecv = 0$. If we write $A$  and $P$ as vectors, this is equivalent to
\[ \vecv \cdot (\vv{P} - \vv{A}) = 0 \]
Usually, this is multiplied out, and the equation of the plane is written in the form
\[ ax + by + cz = d \]
In this case, the normal vector turns out to be $\angled{a,b,c}.$

\subsubsection{Given Three Points}
When given three points $A,B,$ and $C$, we first need to find the normal vector. This can be done by taking $\vv{AB} \times \vv{AC}.$ Then, the rest of the calculation proceeds as shown prior.

\subsubsection{Finding line Given Two Planes}
To find the line of intersection given two planes, first find a point on the line. Then find the normal vectors $\vv{n}$ and $\vv{m}$, and take the cross product, giving a line which lies on both planes.

\subsubsection{Distance From Point To Plane}
We have a plane defined by a point $A$ and a normal vector $\vv{n}$, and a point $B$. Let $d$ be the length of the projection of $\vv{AB}$ onto $\vv{n}$. Note that $d$ will be the desired length, as we can move $B$ horizontally parallel to the plane onto $\vecn$ and conserve the distance to the plane. Then, by simple trigonometry and substitutions we get
\begin{align*}
d & = \frac{\norm{\vecn} \, \norm{\vv{AB}} \, |\cos{\theta}|}{\norm{\vecn}} \\
& = \boxed{\frac{|\vecn \cdot \vv{AB}|}{\norm{\vecn}}}
\end{align*}
as desired.

\section{Parametric Equations}
\subsection{Example}
Assume we have a function
\[ \vv{r}(t) = \angled{\cos{t},\sin{t},t}. \]
The first two coordinates we recognize as the equation for a circle, and the third is simply linear, so the resulting graph is a spiral in the $+z$ direction.
\subsection{Derivatives of Parametric Equations}
The derivative of $r(t) = \angled{x(t),y(t),z(t)}$ is defined
\begin{align*}
\frac{d}{dt} \vv{r}(t) &= \lim_{h \rightarrow 0} \frac{\vv{r}(t+h)-\vv{r}(t)}{h} \\
& = \lim_{h \rightarrow 0} \angled{\frac{x(t+h)-x(t)}{h},\frac{y(t+h)-y(t)}{h},\frac{z(t+h)-z(t)}{h}} \\
& = \angled{x^\prime(t),y^\prime(t),z^\prime(t)}
\end{align*}
If $\vecr(t)$ is position, then $\vecr^\prime(t)$ is velocity and $\vecr^{\prime\prime}(t)$ is acceleration. The magnitude of velocity is speed.

\subsection{Arclength}
Let $s(t)$ be the function for the arclength of a parametric function. Simply,
\[ s(t) = \int_{t_1}^t{\norm{\vecv(\tau)}d\tau}. \]
Additionally,
\begin{align*}
s^\prime(t) & = \frac{d}{dt} \int_{t_1}^t{\norm{\vecv(\tau)}d\tau} \\
& = \norm{\vecv(t)}.
\end{align*}

\subsection{Constant Length}
If $\vecr(\alpha)$ has the property that $\norm{\vecr(/alpha)} = k$, then
\[ \vecr \cdot \vecr = k^2.\]
Differentiating, we get
\[ \vecr^\prime \cdot \vecr + \vecr \cdot \vecr^\prime = 0 \]
so
\[ 2 \vecr \cdot \vecr^\prime = 0 \]
and
\[ \vecr \perp \vecr^\prime.\]

\subsection{Special Unit Vectors}
We define the normal tangent vector as \[\vb{T} = \frac{\vecr^\prime}{\norm{\vecr^\prime}}\] and the unit normal vector as \[\vb{N} = \frac{dT/d\alpha}{\norm{dT/d\alpha}}.\]
The binormal vector is defined as \[\vb{B} = \vb{T} \times \vb{N}. \]
These vectors are all perpendicular to one another, and they create a coordinate system for us to describe motion along a curve.
\subsection{Curvature}
We define curvature as the change in the tangent vector per unit length, or:
\[ \kappa = \norm{\frac{dT}{ds}}. \]
This is intuitively simple, but deceptively hard to calculate! Thankfully, we have the chain rule:
\begin{align*}
    \kappa & = \norm{\frac{dT/d\alpha}{ds/d\alpha}} \\
    & = \norm{\frac{dT/d\alpha}{\norm{\vecv(\alpha)}}}
\end{align*}
So, to calculate the curvature we have to parameterize both the tangent vector and the speed, making differentiation simple. Some notable examples are:
\begin{itemize}
    \item The curvature of a circle is $\frac{1}{r}$.
\end{itemize}
\subsection{Decomposing Acceleration}
We can parameterize acceleration as
\begin{align*}
\vv{a} &= a_tT + a_nN \\
& = (\vv{a} \cdot \vb{T})T + (\vv{a} \cdot N)N \\
& = \left(\ddt \norm{v}\right)T + (\kappa \norm{v}^2)N \\
& = (\text{proj}_T\vv{a})T + (\text{proj}_N\vv{a})N
\end{align*}
$a_T$ is called the scalar component of acceleration in the tangential direction, and $a_N$ is called the normal component of acceleration in the tangential direction. When $a_N=0$, the acceleration is entirely along the axis of motion. When $a_T=0$, the acceleration is orthogonal to the the direction of motion.
\subsubsection{Derivation}
We begin with
\[ v = \norm{v}\vb{T} \] by the definition of the unit tangent vector.
Differentiating with respect to time, we get
\[ a = \left(\ddt \norm{v}\right)\vb{T} + \frac{d \vb{T}}{dt} \norm{v} \]
We also know that $\kappa = \frac{dT/dt}{\norm{v(t)}}$ or $\frac{dT}{dt} = \kappa \norm{v(t)}$, so we get
\[ a = \left(\ddt \norm{v}\right)\vb{T} + (\kappa \norm{v}^2)\vb{N}. \]

\subsection{Integration}
When integrating any parametric or vector function, we take the antiderivative of each component individually.
\begin{align*}
    \int{\angled{t,\sin{t},e^{2t}}dt} &= \angled{\frac{1}{2}t^2,-\cos{t},\frac{1}{2}e^{2t} + \vv{c}} \\
    &= \angled{\frac{1}{2}t^2+c_1,-\cos{t}+c_2,\frac{1}{2}e^{2t}+c_3}
\end{align*}
\subsection{Projectile Motion}
Instead of using kinematic equations, we can solve projectile motion through vectors and integration. As always, we have
\[ \vv{a} = \angled{0,-9.8} \]
Then by integrating we get
\begin{align*}
    \vv{v} = \int{\vv{a}dt} &= \angled{0,-9.8t}+\vv{v_0} \\
    & = \angled{v_{0x},-9.8t + v_{0y}}
\end{align*}
Integrating again, we get
\[ \vecr = \angled{v_{0x}t,-4.9t^2 + v_{0y}t} + \vv{r_0} \]

\section{Functions of Several Variables}
We will now study functions of the type
\[ y:\mathbb{R}^2 \rightarrow \mathbb{R} \; \text{or} \; y:\mathbb{R}^3 \rightarrow \mathbb{R} .\]
Take for example the function $f(x,y) = x^2 + y^2$. This function maps every point on the plane to another point on a number line. This function can also be visualized as $z=x^2+y^2$, now it is a graph in $3$ dimensions which forms a bowl.
\subsection{Limits}
$\frac{x}{x} \neq 1$ but $\lim\limits_{x\rightarrow0} \frac{x}{x} = \lim\limits_{x\rightarrow0} 1$, remember your notation!
Limits are different in two dimensions, there is no left hand side or right hand side. This makes showing the limit exists more difficult. Limits in two dimensions are denotated as
\[ \lim_{(x,y)\to(x_0,y_0)} f(x,y) .\]
Often we prohibit a certain line in these two dimensions, such as in the case
\[ \lim_{(x,y)\to(0,0)} \frac{x^2-y^2}{x-y} \]
we exclude the line $y=x$.
\subsubsection{Delta Epsilon Definition of The Limit}
\[
    \lim_{x\to x_0}f(x) = L \] means that
\[ \forall \epsilon > 0,  \ \exists  \delta > 0 \ni ,\]
\[ 0 < \abs{x-x_0} < \delta \implies \abs{f(x)-L} < \epsilon \]
In English, this says that for all $\epsilon > 0$, we can choose a delta such that if the left bottom statement is true the right bottom statement is also true.

\subsection{Partial Derivatives}
Assume we have a function $f(x,y)$ and we wish to find its derivative. Derivative usually tell us the ratio of the rate of change of $f$ to the rate of change of $x$, our input variable. However, in this case we have multiple input variables. To take the derivative of just one variable we have partial dervatives, which are defined in our case as
\[ \frac{\partial f}{\partial x} = \lim_{h\to 0} \frac{f(x+h,y) - f(x,y)}{h} \]
\[ \frac{\partial f}{\partial y} = \lim_{h\to 0} \frac{f(x,y+h) - f(x,y)}{h} \]
Second partial derivatives are written as
\[ \frac{\partial^2 f}{\partial y \, \partial x} = \frac{\partial}{\partial y}\left(\frac{\partial}{\partial x}(f)\right) \]
\[ \frac{\partial^2 f}{\partial x^2} = \frac{\partial}{\partial x}\left(\frac{\partial}{\partial x}(f)\right) \]

Also, \[\frac{\partial^2 f}{\partial y \, \partial x} = \frac{\partial^2 f}{\partial x \, \partial y}\]


\subsubsection{Linearity}
A function $f$ is linear if the following are true:
\begin{itemize}
    \item $f(kx) = kf(x)$
    \item $f(x+y) = f(x) + f(y).$
\end{itemize}
Therefore not all lines are linear!
The derivative operator is linear because
\[ \ddx (f(x)+g(x)) = \frac{df}{dx} (x) + \frac{dg}{dx} (x) \]
\[ \ddx kf(x) = k \frac{df}{dx}(x) .\]

\subsubsection{Notation}
\begin{itemize}
    \item $\frac{\partial f}{\partial x} = f_x$
    \item $\frac{\partial f}{\partial y} = f_y$
    \item $\frac{\partial^2 f}{\partial y \partial x} = f_{xy}$
    \item $\frac{\partial^4 f}{\partial z \partial w^2 \partial x} = f_{xwwz}$
\end{itemize}

\subsection{Chain Rule}
Assume we have a function $f(x(t),y(t)) = z.$ Note that $f$ can be written as either a function of just $t$ or also a function of $x(t)$ and $y(t)$.
Then the Chain Rule states that
\begin{align*}
\frac{df}{dt} & = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}  \\
&= \angled{\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}} \cdot \angled{\frac{dx}{dt},\frac{dy}{dt}} \\
& = \ddt f\left(\vecr(t)\right)
\end{align*}
$\angled{\frac{\partial f}{\partial x}, \frac{\partial f}{\partial x}} = \nabla f(x,y)$, or the gradient (more about this later), so the second line becomes
\[ \frac{df}{dt} = \nabla f(x,y) \cdot r^\prime. \]
If we have $f(x(u,w),y(u,w),z(u,w)$, to take the derivative we do
\begin{align*}
    \frac{\partial f}{\partial u} & =
    \frac{\partial f}{\partial x} \frac{\partial x}{\partial u} +
    \frac{\partial f}{\partial y} \frac{\partial y}{\partial u} +
    \frac{\partial f}{\partial z} \frac{\partial z}{\partial u} \\
    & = \angled{
    \frac{\partial f}{\partial x},
    \frac{\partial f}{\partial y},
    \frac{\partial f}{\partial z}} \cdot
    \angled{
    \frac{\partial x}{\partial u},
    \frac{\partial y}{\partial u},
    \frac{\partial z}{\partial u}} \\
    & = \nabla f(x,y,z) \cdot
    \angled{
    \frac{\partial x}{\partial u},
    \frac{\partial y}{\partial u},
    \frac{\partial z}{\partial u}}
\end{align*}

\subsection{Level Curves}
The level curve for $f(x,y)=k$ is the set of all soutions to this equation.
They can be visualized in $2$ variables as a plane slicing the function at $z=k$.
If we let $f(\vecr(t))=k$ where $\vecr(t)$ describes all point on the level curve, then differentiating gives us
\[ \nabla f(\vecr(t)) \cdot \vecr^\prime(t) = 0 \]
by the chain rule. This means that these two vectors are orthogonal. Therefore, \textbf{the gradient at a point is always perpendicular to the tangent line of the level curve at that point.}

\subsection{Directional Derivatives}
We wish to find the rate of change of some function $f(x,y)$ in a particular direction $\vv{d} = \angled{d_1,d_2}.$ Using the limit definition of the derivative, this turns out to be
\begin{align*} \lim_{h\to 0} \frac{f(x+hd_1,y+hd_2)-f(x,y)}{h} & =
\frac{d}{dh} f\left(\angled{x,y} + h\vv{d}\right) \Bigr\rvert_{h = 0} \\
& = \nabla f\left(\angled{x,y} + h \vv{d}\right) \cdot \vv{d} \Bigr\rvert_{h = 0} \\
&= \nabla f(x,y) \cdot \vv{d}
\end{align*}
This is called the directional derivative. This also means that the gradient is the direction of steepest increase, since the above function is maximized when $\vv{d}$ is in the same direction as the gradient. As the function changes faster, the gradient increases.

\subsection{Matrices}
When multiplying matrices, we multiply rows by columns. For example:
\[ \begin{bmatrix}
        2 & -1 \\
        4 & 1
    \end{bmatrix}
    \begin{bmatrix}
        3 \\ 5
    \end{bmatrix} =
    \begin{bmatrix}
        2\cdot3 + -1 \cdot 5 \\ 4 \cdot 3 + 1 \cdot 5
    \end{bmatrix} =
    \begin{bmatrix}
        1 \\ 17
    \end{bmatrix}
\]
To multiply two matrices, the first must have the same number of columns as the second matrix has rows.
\subsection{Hessian}
The Hessian of a function is denoted as $\nabla^2 f.$ or $H_f$ The Hessian of a function is
\[ \begin{bmatrix}
    \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
    \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}. \]
Recall that the gradient is \[\angled{\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}}.\] We obtain the Hessian by taking the partial derivatives of $x$ and $y$ of each component of the gradient. The Hessian gives us all the second order information about a function.

\subsection{Taylor Expansions}
Recall from Calculus II that the Taylor expansion is
\[ f(x) \approx \overbrace{\underbrace{f(x_0) + f^\prime(x_0)(x-x_0)}_{\text{First order Taylor polynomial}} + \frac{1}{2} f^{\prime \prime}(x_0)(x-x_0) + \cdots}^{\text{Function of x,}\  y=T(x)}\]
This says that we can perfectly approximate any function given perfect information about a single point and all the derivatives at that point.
\subsubsection{Example}
We can use the Taylor expansion to gain intuition on some elementary derivatives:
\begin{align*}
    \ddx e^x &= \ddx \left( 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \dots \right) \\
    &= 0 + 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots \\
    &= e^x
\end{align*}
\subsubsection{Taylor In 2 Variables}
In $2$ dimensions, we have
\[ f(x,y) = f(x_0,y_0) + \nabla f(x_0,y_0) \cdot \angled{x-x_0,y-y_0} + \half \underbrace{\angled{x-x_0,y-y_0}}_{1 \times 2 \text{ matrix}} \nabla^2 f(x_0,y_0) \begin{bmatrix} x-x_0 \\ y-y_0 \end{bmatrix} + \cdots. \]
This is as far as we will go. Note that the matrix multiplication of the last term all cancels out and we are left with a scalar term.
\subsection{Tangent Planes}
We know from our Taylor expansion that the tangent plane at a point should be
\[ z = f(x_0,y_0) + \nabla f(x_0,y_0) \cdot \angled{x-x_0,y-y_0}. \]
We can now rewrite this as
\begin{align*}
    0 &= -(z-z_0) + \angled{f_x,f_y} \cdot \angled{x-x_0,y-y_0} \\
    0 &= \angled{f_x,f_y,-1} \cdot \angled{x-x_0,y-y_0,z-z_0}
\end{align*}
Note that the vector $\angled{f_x,f_y,-1}$ is the normal vector to the plane, and therefore also the coefficients of the tangent plane in standard form.
\subsubsection{Level Surfaces}
The tangent plane to a level surface $f(x,y,z) = k$ is
\[  \frac{\partial f}{\partial x} (x-x_0) +
    \frac{\partial f}{\partial y} (y-y_0) +
    \frac{\partial f}{\partial z} (z-z_0) = 0\]
or alternatively,
\[  \angled{\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}} \cdot \left( \angled{x,y,z}-\angled{x_0,y_0,z_0} \right) = 0\]
\section{Optimization}
There are two cases of optimization:
\begin{itemize}
    \item (Easier) Unconstrained
    \item (Harder) Constrained
\end{itemize}
\subsection{Unconstrained optimization}
Given a function $z=f(x,y)$ we see that if we construct a tangent plane at the local maximums and minimums, the normal vectors at those points will be $\angled{0,0,-1}$. Therefore, local max/min $\implies \nabla f(x,y) = \angled{0,0}.$ Critical points are solutions to the equation $\nabla f(x,y) = \angled{0,0}$. These can either be local minimums, local maximums, or saddle points.
\subsubsection{Classification}
We can classify critical points based on the Hessian $(\nabla^2 f)$ at that point. Assume the Hessian is diagonalizable into
\[ \begin{bmatrix}
    \lambda_1 & 0 \\
    0 & \lambda_2 \\
\end{bmatrix} \]
$\lambda_1$ and $\lambda_2$ are called eigenvalues. Then,
\begin{itemize}
    \item Both positive $\implies$ local minimum
    \begin{itemize}
        \item Bending upwards in both directions
    \end{itemize}
    \item Both negative $\implies$ local maximum
    \begin{itemize}
        \item Bending downwards in both directions
    \end{itemize}
    \item Split signs $\implies$ saddle point
    \begin{itemize}
        \item Upwards in one direction, positive in the other direction.
    \end{itemize}
    \item $\lambda_1 \cdot \lambda_2 = 0 \implies$ indeterminate
\end{itemize}
Alternatively, we can classify them by the determinant of the Hessian (note that this will give mostly the same results as using the eigenvalues because the determinant of the diagonalized Hessian is the product of the eigenvalues.)
\begin{itemize}
    \item det$\left( \nabla^2 f(x^*,y^*)\right)<0 \implies$ saddle point.
    \item det$\left( \nabla^2 f(x^*,y^*)\right)>0$ and $\frac{\partial^2 f}{\partial x^2}>0 \implies$ local minimum.
    \item det$\left( \nabla^2 f(x^*,y^*)\right)>0$ and $\frac{\partial^2 f}{\partial x^2}<0 \implies$ local maximum.
    \item det$\left( \nabla^2 f(x^*,y^*)\right)=0 \implies$ inconclusive.
\end{itemize}
\subsection{Constrained Optimization}
Imagine we have some function $f(x)$ to minimize,
where $x = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}.$ Additionally, imagine there are a set of constraints on $x$, such as $h(x) = 0$ and $g(x) \leq 0$. Using all this information together, we can put our problem in canonical form, which is:
\begin{align*}
    \min \  &f(x) \text{ such that} \\
    &h(x) = 0 \\
    &g(x) \geq 0
\end{align*}
To do this, we construct the Langrangian, which is
\[ \Lagr \left(x,\lambda,\rho \right) = f(x) - \lambda h(x) -\rho g(x). \]
\begin{claim}
    $\min\limits_x\left(\max\limits_{\lambda,\rho\geq0}  f(x) - \lambda h(x) -\rho g(x)\right) = \min f(x)$ such that $h(x) = 0 $ and $g(x) \geq 0$.
\end{claim}
\begin{proof}
We first try to break the constraints. If we choose $x$ such that $h(x) \neq 0$, then $\lambda$ will be chosen so that the maximum of the function goes to $\infty$, so this is not our minimum. If we choose $x$ such that $g(x) < 0$, then $\rho \to \infty$ and $\Lagr \to \infty$. We now know the conditions must be met, so $\left(\max\limits_{\lambda,\rho\geq0}  f(x) - \lambda h(x) -\rho g(x)\right) = f(x)$ because $h(x) = 0$ and $\rho = 0$. Therefore, the left hand side in our claim simply becomes $\min\limits_x f(x)$ under the conditions as desired.
\end{proof}
We can also imagine this as Player $1$ wants to minimize $\Lagr$ and chooses an $x$ first, and then Player $2$ wants to maximize $\Lagr$ and chooses $\lambda$ and $\rho \geq 0$.

Solving for the minimum gives us the following list of conditions:
\begin{itemize}
    \item $\nabla_x \Lagr(x,\lambda,\rho) = 0$
    \item $h(x) = 0$, $g(x) \geq 0$ % Not sure if this is right? it might be lambda = 0
    \item $\rho g(x) = 0, \rho \geq 0$.
\end{itemize}
\subsubsection{Example Problem}
Find the point on the plane $3x+5y+7z=19$ closest to the point $\left( 0,2,-4 \right)$.

First, we model the the problem. We want $\min \norm{\angled{x,y,z}-\angled{0,2,-4}}^2$, such that $3x+5y+7z=19$.

Next, we construct the Lagrangian.
$\Lagr(x,y,z,\lambda) = \norm{\angled{x,y-2,z+4}}^2 - \lambda (3x+5y+7z-19)$.

Our necessary conditions are:
\begin{itemize}
    \item $\nabla_{x,y,z} \Lagr(x,y,z,\lambda) = \angled{0,0,0}$
    \item $3x+5y+7z-19=0$
\end{itemize}
This now becomes a system of equations, and can be trivially solved.

\subsection{Lagrange Multipliers}
Assume we have some limitation $h(x,y)=0.$ We wish to minimize $f(x,y)$ such that $h(x,y)=0$. This creates the Lagrangian $\Lagr(x,y,\lambda) = f(x,y)-\lambda h(x,y)$. Then
\[ \nabla \Lagr(x,y,\lambda) = \angled{\frac{\partial f}{\partial x} - \lambda \frac{\partial h}{\partial x}, \frac{\partial f}{\partial y} - \lambda \frac{\partial h}{\partial y}, -h} = \angled{0,0,0}.\]
Then we get the system of equations
\begin{align*}
    \frac{\partial f}{\partial x} - \lambda \frac{\partial h}{\partial x} &= 0 \\
    \frac{\partial f}{\partial y} - \lambda \frac{\partial h}{\partial y} &= 0 \\
    -h &= 0.
\end{align*}
Therefore,
\[ \nabla f = \lambda \nabla h. \]
This means that the gradient of $f(x,y)$ is parallel to the gradient of $h(x,y)$ at the minimum.

\section{Iterated Integrals}
We wish to find the volume under some surface $f(x,y)$ from $x \in \left[a,b\right]$ to $y \in \left[c,d\right]$. Let this rectangular area be $R$. This volume can then be expressed as
\[ \int \int_R f(x,y) \, \dd{A} = \int_a^b \int_c^d f(x,y) \, \dd{y} \dd{x} = \int_c^d \int_a^b f(x,y) \, \dd{x} \dd{y}\]
We can switch the order of integration by Fubini's Theorem.

\subsection{Complex Regions}
Sometimes we may need to integrate a function $f(x,y)$ over a non-rectangular region, such as a triangle. In this case, we must set our limits carefully. Imagine a triangle bounded by the lines $x=2$, $y=0$, and $y=x$. Then our integral becomes
\[ \int_0^2 \int_0^x f(x,y) \, \dd{y} \dd{x}.\]
Reversing the order of integration, we consider the geometry of the region and find that it becomes
\[ \int_0^2 \int_y^2 f(x,y) \, \dd{x} \dd{y}.\]

\subsection{Calculating Areas}
To calculate the area of a region $R$, we evaluate the integral
\[ \int \int_R 1 \dd{A} .\]
This lets us calculate the average value of a function over a region by evaluating the integral
    \[ \frac{\int \int_R f(x,y) \dd{A}}{\int \int_R \dd{A}} .\]
\subsection{Splitting Regions}
If we have non-overlapping regions $R = R_1 \cup R_2$ such that $R_1 \cap R_2 = \varnothing$ then
\[\int\int_R f(x,y) \dd{A} = \int\int_{R_1} f(x,y) \dd{A} + \int\int_{R_2} f(x,y) \dd{A}.\]

\subsection{Polar Regions}
To start, we wish to use double integrals to find the area in some polar function $r(\theta)$. We know the area is $\int\int_R \dd{A}$, so we need to find this in polar coordinates. We remember that
\[ \begin{pmatrix}
    x \\ y
\end{pmatrix} \mapsto \begin{pmatrix}
    r\cos{\theta} \\ r\sin{\theta}
\end{pmatrix}. \]
Differentiating this with respect to our two variables $r$ and $\theta$ gives us the Jacobian matrix $\mathbf{J}$ of $r(\theta)$
\[ \begin{bmatrix}
    \cos{\theta} & -r\sin{\theta} \\
    \sin{\theta} & r\cos{\theta}
\end{bmatrix}. \]
Then $\dd{A} = \dd{x}\dd{y} = \text{det}(\mathbf{J})\dd{r}\dd{\theta} = r\dd{r}\dd{\theta}$

\subsubsection{Jacobians}
The Jacobian is a matrix of all the first-order partial derivatives of a vector function in the form
\[ \begin{bmatrix}
    \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
    \vdots                             & \ddots & \vdots\\
    \dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n}
\end{bmatrix}. \]
In the case of $r(\theta)$, $f_1(\theta) = r\cos{\theta}$ and $f_2(\theta) = r\sin{\theta}$.
The Jacobian matrix measures some stuff \footnote{Wikipedia says it measures the amount of stretching that the coordinate change imposes} when you change from coordinate space $(x,y)$ to another coordinate space $(u,v)$. This is useful to us because in two dimensions, the determinant of the Jacobian is $\frac{\dd{u}\dd{v}}{\dd{x}\dd{y}}$, allowing us to change variables in double integrals.

\subsection{Triple Integrals}
Triple integrals are simply a dimension higher than double integrals. The volume of a domain is simply
\[ V = \int\int\int_D \dd{V} \]
and the average value of a function over a domain is
\[ \frac{\int\int\int_D f(x,y,z) \dd{V}}{\int\int\int_D \dd{V}} \]
where $\dd{V} = \dd{x}\dd{y}\dd{z}$.

\subsection{Applications}
\subsubsection{Center of Mass}
For any object, you can find each coordinate of the center of mass with the formula
\[ \vv{x} = \frac{\int\int_R x \, \delta(x,y) \dd{A}}{\int\int_R \delta(x,y) \dd{A}} \]
where $\delta(x,y)$ is the density at a point.
Note that the total mass is just \[\int\int_R \delta(x,y) \dd{A}.\]

\subsection{Angular Motion}
Let the equation of a object in circular motion be
\[ r\left(\theta\right) = \angled{d\cos{\theta},d\sin{\theta}}.\]
We can now calculate the speed of this object!
\begin{align*}
    \dv{r}{t} &= \angled{-d\sin{\theta}\dv{\theta}{t},d\cos{\theta}\dv{\theta}{t}}\\
    \norm{\dv{r}{t}} &= d\dv{\theta}{t} \\ &= d\omega.
\end{align*}
Now the total energy when we rotate some $3$-dimensional shape is
\begin{align*}
    &\int\int\int_R \half d^2(x,y,z)\omega^2 \, \delta(x,y,z)\dd{V} \\
    = \half \omega^2 &\underbrace{\int\int\int_R d^2(x,y,z) \, \delta(x,y,z) \dd{V}}_{\text{Moment of intertia}}
\end{align*}
Note the similarities to the equation $E=\half mv^2$. $\omega$ replaces $v$, and moment of inertia replaces $m$.

\subsection{Three Dimensional Coordinate Systems}
We end \footnote{almost} as we started, with coordinate systems.
\subsubsection{Rectangular Coordinates}
Our regular cartesian coordinate system. Coordinates are $(x,y,z)$, you know this.

\subsubsection{Cylindrical Coordinates}
Coordinates are $(r,\theta,z)$. The $xy$ plane is replaced with polar coordinates. Note that $r$ is not truly the distance from the point to the origin. The Jacobian of the mapping
\[ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \mapsto \begin{pmatrix} r\cos{\theta} \\ r\sin{\theta} \\ z \end{pmatrix} \]
is
\[ \mathbf{J} =
\begin{bmatrix}
    \cos{\theta} & -r\sin{\theta} & 0 \\
    \sin{\theta} & r\cos{\theta} & 0 \\
    0 & 0 & 1
\end{bmatrix} \]
The determinant of this is simply $r$ as shown in section \ref{3x3d}, so
\[ \dd{V} = r \dd{z} \dd{r} \dd{\theta} \]

\subsubsection{Spherical Coordinates}
Coordinates are $(\rho,\theta,\phi)$. Imagine $\vv{v}$ is our vector from the origin to the desired point. Then $\phi$ is the angle from the $z\footnote{z \text{doesn't really exist here but we will pretend it does.}}$-axis to $\vv{v}$. $\rho$ is the length of $\vv{v}$, and $\theta$ is the angle from the $x$-axis to the projection of $\vv{v}$ onto the $xy$-plane. The Jacobian of the mapping
\[ \begin{pmatrix} x \\ y \\ z \end{pmatrix} \mapsto \begin{pmatrix} \rho\sin{\phi}\cos{\theta} \\ \rho\sin{\phi}\sin{\theta} \\ \rho\sin{\phi} \end{pmatrix} \]
is
\begin{align*} \mathbf{J} &=
\begin{bmatrix}
    \sin{\phi}\cos{\theta} & \rho\cos{\phi}\cos{\theta} & -\rho\sin{\phi}\sin{\theta} \\
    \rho\sin{\phi}\sin{\theta} & \rho\cos{\phi}\sin{\theta} & \rho\sin{\phi}\cos{\theta} \\
    \cos{\theta} & -\rho\sin{\phi} & 0
\end{bmatrix}  \\
\det{\mathbf{J}} &= \rho^2\sin{\phi}.
\end{align*}
Therefore,
\[ \dd{V} = \left|\rho^2\sin{\phi}\right|\dd{\rho}\dd{\phi}\dd{\theta}. \]

\subsection{Line Integrals}
We have a curve $C$ on the $xy$ plane and a function $f(x,y)$. The line integral is the area under the curtain formed by the heights of $f$ across the curve $C$. This integral can be expressed as
\[ \int_C f(x,y) \dd{s} \]
where $\dd{s}$ is the differential of arclength. Recall that
\begin{align*}
    \dv{s}{t} &= \norm{v} \\
    \dd{s} &= \dv{s}{t} \dd{t} \\
    &= \norm{v} \dd{t}.
\end{align*}
So our previous integral becomes
\[ \int_a^b f(x(t),y(t)) \norm{v} \dd{t}. \]
\begin{remark}
    We can write lines from point $\angled{x_0,y_0}$ to $\angled{x_1,y_1}$ with the formula
    \[\ell(t) = (1-t)\angled{1,3} t\angled{4,6}.\]
\end{remark}


















\end{document}
